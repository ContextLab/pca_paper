{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: davos in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: packaging in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.9/site-packages (from davos) (23.0)\n",
      "Requirement already satisfied: setuptools in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.9/site-packages (from davos) (65.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.1.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install davos\n",
    "import davos\n",
    "\n",
    "%conda install pytables\n",
    "\n",
    "davos.config.suppress_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smuggle nltools as nlt            # pip: nltools==0.4.7\n",
    "\n",
    "smuggle nilearn as nl             # pip: nilearn==0.10.0\n",
    "smuggle nibabel as nib            # pip: nibabel==5.0.1\n",
    "\n",
    "smuggle datawrangler as dw        # pip: pydata-wrangler==0.2.2\n",
    "\n",
    "smuggle numpy as np               # pip: numpy==1.24.2\n",
    "smuggle matplotlib.pyplot as plt  # pip: matplotlib==3.7.0\n",
    "smuggle pandas as pd              # pip: pandas==1.5.3\n",
    "smuggle seaborn as sns            # pip: seaborn==0.12.2\n",
    "from skimage smuggle transform    # pip: scikit-image==0.20.0\n",
    "\n",
    "from sklearn.decomposition smuggle IncrementalPCA as PCA  # pip: scikit-learn==1.2.1\n",
    "from scipy.spatial.distance smuggle cdist                 # pip: scipy==1.10.1\n",
    "from scipy.io smuggle loadmat\n",
    "from tqdm smuggle tqdm            # pip: tqdm==4.64.1\n",
    "\n",
    "smuggle requests                  # pip: requests==2.28.2\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import warnings\n",
    "from glob import glob as lsdir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "datadir = os.path.join(basedir, 'data')\n",
    "nii_dir = os.path.join(datadir, 'nii')\n",
    "\n",
    "zipfile_fname = os.path.join(nii_dir, 'Pieman2.zip')\n",
    "check_file = os.path.join(nii_dir, 'download_complete.txt')\n",
    "url = 'https://www.dropbox.com/s/0uof8a3t3xxhxcm/pieman_archive.tar.gz?dl=1'\n",
    "# url = 'https://www.dropbox.com/s/0g5nx37p64eubif/pieman_posterior_K700.mat?dl=1'\n",
    "\n",
    "if not os.path.exists(check_file):\n",
    "    if not os.path.exists(nii_dir):\n",
    "        os.makedirs(nii_dir)\n",
    "    \n",
    "    if not os.path.exists(zipfile_fname):\n",
    "        with open(zipfile_fname, 'wb') as f:\n",
    "            data = requests.get(url).content\n",
    "            f.write(data)\n",
    "\n",
    "        shutil.unpack_archive(zipfile_fname, nii_dir)\n",
    "        os.remove(zipfile_fname)\n",
    "\n",
    "        with open(check_file, 'w+') as f:\n",
    "            f.write(str(dt.now()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jmanning/pca_paper/data/nii'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nii_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_files = lsdir(os.path.join(nii_dir, 'new_pieman', '*', 'func', '*.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx1 = os.path.join(datadir, 'pieman2.pkl')\n",
    "\n",
    "if os.path.exists(fx1):\n",
    "    with open(fx1, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "else:\n",
    "    data = {os.path.split(f)[-1][:-len('.nii')]: nlt.data.Brain_Data(data=f) for f in tqdm(nii_files)}\n",
    "\n",
    "    with open(os.path.join(datadir, 'pieman2.pkl'), 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2 = {os.path.split(nii_files[i])[-1][:-len('.nii')]: v for i, v in enumerate(data.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(datadir, 'pieman2.pkl'), 'wb') as f:\n",
    "#         pickle.dump(data2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subj2cond(x):\n",
    "    if 'intact' in x:\n",
    "        return 'intact'\n",
    "    elif 'paragraph' in x:\n",
    "        return 'paragraph'\n",
    "    elif 'word' in x:\n",
    "        return 'word'\n",
    "    elif 'rest' in x:\n",
    "        return 'rest'\n",
    "    else:\n",
    "        raise Exception('Unknown condition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 912750.09it/s]\n"
     ]
    }
   ],
   "source": [
    "fx2 = os.path.join(datadir, 'pieman2_organized.pkl')\n",
    "\n",
    "if os.path.exists(fx2):\n",
    "    with open(fx2, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "else:\n",
    "    x = {}\n",
    "    for k, v in tqdm(data.items()):\n",
    "        if subj2cond(k) in x:\n",
    "            x[subj2cond(k)].append(v)\n",
    "        else:\n",
    "            x[subj2cond(k)] = [v]\n",
    "\n",
    "    with open(fx2, 'wb') as f:\n",
    "        pickle.dump(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['intact', 'paragraph', 'word', 'rest']\n",
    "\n",
    "# def load_data(fname):\n",
    "#     htfa = loadmat(fname, simplify_cells=True)['posterior']\n",
    "#     weights = [pd.DataFrame(htfa['subjects'][i]['image_weights']['mean']) for i in range(len(htfa['subjects']))]\n",
    "#     data = {}    \n",
    "#     for c in conditions:\n",
    "#         data[c] = [w for w, n in zip(weights, htfa['subjects_names']) if c in n]\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# data = load_data(fname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load (computing and saving as needed) the reduced versions of the data using $k \\in \\{3, 4, 5, ..., 700\\}$ components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_pca(x, n_components=None):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    y = pca.fit_transform(x)\n",
    "\n",
    "    return dw.unstack(pd.DataFrame(index=x.index, data=y))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 33/1000 [15:16:24<535:07:16, 1992.18s/it]"
     ]
    }
   ],
   "source": [
    "scratch_dir = os.path.join(basedir, 'data', 'scratch', 'voxelwise')\n",
    "if not os.path.exists(scratch_dir):\n",
    "    os.makedirs(scratch_dir)\n",
    "\n",
    "reduced_data = {}\n",
    "max_components = x['intact'][0].data.shape[1]\n",
    "components = np.linspace(3, max_components, 100).astype(int)\n",
    "\n",
    "for c in conditions:\n",
    "    reduced_data[c] = {}\n",
    "    stacked_data = dw.stack([i.data for i in x[c]])\n",
    "    for n in tqdm(components):\n",
    "        fname = os.path.join(scratch_dir, f'pca_{c}_{n}.pkl')        \n",
    "\n",
    "        if not os.path.exists(fname):\n",
    "            reduced_data[c][n] = group_pca(stacked_data, n_components=n)\n",
    "\n",
    "            with open(fname, 'wb') as f:\n",
    "                pickle.dump(reduced_data[c][n], f)\n",
    "\n",
    "        with open(fname, 'rb') as f:\n",
    "            reduced_data[c][n] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(train, test):\n",
    "    train = np.mean(np.stack(train, axis=2), axis=2)\n",
    "    test = np.mean(np.stack(test, axis=2), axis=2)\n",
    "    dists = cdist(train, test, metric='correlation')\n",
    "    \n",
    "    labels = np.argmin(dists, axis=1)\n",
    "    return np.mean([i == d for i, d in enumerate(labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data, n_folds=100):\n",
    "    results = pd.DataFrame(columns=['Fold', 'Number of components', 'Decoding accuracy'])\n",
    "\n",
    "    n = len(data[3]) // 2\n",
    "    for i in tqdm(range(n_folds)):\n",
    "        order = np.random.permutation(len(data[3]))\n",
    "\n",
    "        for c in range(3, max_components + 1):\n",
    "            x = pd.DataFrame(columns=['Fold', 'Number of components', 'Decoding accuracy'])\n",
    "            x.loc[0, 'Fold'] = i\n",
    "            x.loc[0, 'Number of components'] = c\n",
    "\n",
    "            train = [data[c][o] for o in order[:n]]\n",
    "            test = [data[c][o] for o in order[n:]]\n",
    "            x.loc[0, 'Decoding accuracy'] = (accuracy(train, test) + accuracy(test, train)) / 2\n",
    "\n",
    "            results = pd.concat([results, x], ignore_index=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [9:44:13<00:00, 350.54s/it]  \n",
      "  8%|▊         | 8/100 [1:20:41<15:37:13, 611.23s/it]"
     ]
    }
   ],
   "source": [
    "decoding_results = {}\n",
    "for c in conditions:\n",
    "    decoding_results[c] = cross_validation(reduced_data[c])\n",
    "    sns.lineplot(decoding_results[c], x='Number of components', y='Decoding accuracy', label=c.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2374a5d97dbff2383fb72c506230f21570e3388024304ec3ac10b13478d174bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
