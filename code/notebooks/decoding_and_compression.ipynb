{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: davos in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: setuptools in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.9/site-packages (from davos) (65.6.3)\n",
      "Requirement already satisfied: packaging in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.9/site-packages (from davos) (23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install davos\n",
    "import davos\n",
    "\n",
    "davos.config.suppress_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smuggle datawrangler as dw        # pip: pydata-wrangler==0.2.2\n",
    "\n",
    "smuggle numpy as np               # pip: numpy==1.24.2\n",
    "smuggle matplotlib.pyplot as plt  # pip: matplotlib==3.7.0\n",
    "smuggle pandas as pd              # pip: pandas==1.5.3\n",
    "smuggle seaborn as sns            # pip: seaborn==0.12.2\n",
    "from skimage smuggle transform    # pip: scikit-image==0.20.0\n",
    "\n",
    "from sklearn.decomposition smuggle IncrementalPCA as PCA  # pip: scikit-learn==1.2.1\n",
    "from scipy.spatial.distance smuggle cdist                 # pip: scipy==1.10.1\n",
    "from scipy.io smuggle loadmat\n",
    "from tqdm smuggle tqdm            # pip: tqdm==4.64.1\n",
    "\n",
    "smuggle requests                  # pip: requests==2.28.2\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "datadir = os.path.join(basedir, 'data')\n",
    "\n",
    "url = 'https://www.dropbox.com/s/29a48lv3j5ybcvw/pieman2_htfa.pkl?dl=1'\n",
    "\n",
    "fname = os.path.join(datadir, 'pieman2_htfa.pkl')\n",
    "if not os.path.exists(fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        data = requests.get(url).content\n",
    "        f.write(data)\n",
    "\n",
    "with open(fname, 'rb') as f:\n",
    "    data = pickle.load(open(fname, 'rb'))\n",
    "\n",
    "conditions = list(data['weights'].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load (computing and saving as needed) the reduced versions of the data using $k \\in \\{3, 4, 5, ..., 700\\}$ components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_pca(data, n_components=None, fname=None):\n",
    "    if fname is not None:\n",
    "        if os.path.exists(fname):\n",
    "            with open(fname, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    x = dw.stack(data)\n",
    "    y = pca.fit_transform(x)\n",
    "\n",
    "    y = dw.unstack(pd.DataFrame(index=x.index, data=y))\n",
    "\n",
    "    if fname is not None:\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump((y, pca), f)\n",
    "    \n",
    "    return y, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698/698 [57:02<00:00,  4.90s/it]\n",
      "100%|██████████| 698/698 [33:15<00:00,  2.86s/it]\n",
      "100%|██████████| 698/698 [59:31<00:00,  5.12s/it]\n",
      " 40%|████      | 282/698 [26:05<37:00,  5.34s/it]  "
     ]
    }
   ],
   "source": [
    "scratch_dir = os.path.join(basedir, 'data', 'scratch')\n",
    "if not os.path.exists(scratch_dir):\n",
    "    os.makedirs(scratch_dir)\n",
    "\n",
    "reduced_data = {'full': {}}\n",
    "max_components = data['weights']['intact'][0].shape[1]\n",
    "\n",
    "n_parts = 4\n",
    "for i in range(n_parts):\n",
    "    reduced_data[f'pt{i}'] = {}\n",
    "\n",
    "for c in conditions:\n",
    "    reduced_data['full'][c] = {}\n",
    "\n",
    "    for i in range(n_parts):\n",
    "        reduced_data[f'pt{i}'][c] = {}\n",
    "    \n",
    "    chunk_size = data['weights'][c][0].shape[0] // n_parts\n",
    "\n",
    "    for n in tqdm(range(3, max_components + 1)):\n",
    "        # full time range\n",
    "        fname = os.path.join(scratch_dir, f'pca_full_{c}_{n}.pkl')\n",
    "        reduced_data['full'][c][n], _ = group_pca(data['weights'][c], n_components=n, fname=fname)\n",
    "        \n",
    "        # chunked time range\n",
    "        for i in range(n_parts):\n",
    "            fname = os.path.join(scratch_dir, f'pca_pt{i + 1}_of_{n_parts}_{c}_{n}.pkl')\n",
    "            chunk_start = i * chunk_size\n",
    "            if i < n_parts - 1:                \n",
    "                chunk_end = (i + 1) * chunk_size\n",
    "            else:\n",
    "                chunk_end = data['weights'][c][0].shape[0]\n",
    "            \n",
    "            next_chunk = [x.iloc[chunk_start:chunk_end] for x in data['weights'][c]]\n",
    "            reduced_data[f'pt{i}'][c][n], _ = group_pca(next_chunk, n_components=n, fname=fname)\n",
    "    \n",
    "    data['weights'].pop(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(train, test):\n",
    "    train = np.mean(np.stack(train, axis=2), axis=2)\n",
    "    test = np.mean(np.stack(test, axis=2), axis=2)\n",
    "    dists = cdist(train, test, metric='correlation')\n",
    "    \n",
    "    labels = np.argmin(dists, axis=1)\n",
    "    return np.mean([i == d for i, d in enumerate(labels)]) - 1 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data, n_iter=10, fname=None):\n",
    "    if fname is not None:\n",
    "        if os.path.exists(fname):\n",
    "            with open(fname, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "\n",
    "    results = pd.DataFrame(columns=['Fold', 'Number of components', 'Relative decoding accuracy'])\n",
    "\n",
    "    n = len(data[3]) // 2\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        order = np.random.permutation(len(data[3]))\n",
    "\n",
    "        for c in range(3, max_components + 1):\n",
    "            x = pd.DataFrame(columns=['Fold', 'Number of components', 'Relative decoding accuracy'])\n",
    "            x.loc[0, 'Iteration'] = i\n",
    "            x.loc[0, 'Number of components'] = c\n",
    "\n",
    "            train = [data[c][o] for o in order[:n]]\n",
    "            test = [data[c][o] for o in order[n:]]\n",
    "            x.loc[0, 'Relative decoding accuracy'] = (accuracy(train, test) + accuracy(test, train)) / 2\n",
    "\n",
    "            results = pd.concat([results, x], ignore_index=True)\n",
    "    \n",
    "    if fname is not None:\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_colors = {\n",
    "    'intact': '#21409A',\n",
    "    'paragraph': '#00A14B',\n",
    "    'word': '#FFDE17',\n",
    "    'rest': '#7F3F98'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "\n",
    "decoding_results = {'full': {}}\n",
    "for i in range(n_parts):\n",
    "    decoding_results[f'pt{i}'] = {}\n",
    "\n",
    "for c in conditions:\n",
    "    # full time range\n",
    "    fname = os.path.join(scratch_dir, f'decoding_results_{n_iter}_full_{c}.pkl')\n",
    "    decoding_results['full'][c] = cross_validation(reduced_data['full'][c], n_iter=n_iter, fname=fname)\n",
    "\n",
    "    # chunked time range\n",
    "    for i in range(n_parts):\n",
    "        fname = os.path.join(scratch_dir, f'decoding_results_{n_iter}_pt{i + 1}_of_{n_parts}_{c}.pkl')\n",
    "        decoding_results[f'pt{i}'][c] = cross_validation(reduced_data[f'pt{i}'][c], n_iter=n_iter, fname=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in conditions:\n",
    "    sns.lineplot(decoding_results['full'][c], x='Number of components', y='Relative decoding accuracy', label=c.capitalize(), color=condition_colors[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_parts):\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for c in conditions:\n",
    "        sns.lineplot(decoding_results[f'pt{i}'][c], x='Number of components', y='Relative decoding accuracy', label=c.capitalize(), color=condition_colors[c])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2374a5d97dbff2383fb72c506230f21570e3388024304ec3ac10b13478d174bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
