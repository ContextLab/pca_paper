{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: davos in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: setuptools in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.10/site-packages (from davos) (65.6.3)\n",
      "Requirement already satisfied: packaging in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.10/site-packages (from davos) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jmanning/opt/anaconda3/envs/pca-paper/lib/python3.10/site-packages (from packaging->davos) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install davos\n",
    "import davos\n",
    "\n",
    "davos.config.suppress_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smuggle numpy as np               # pip: numpy==1.24.2\n",
    "smuggle matplotlib.pyplot as plt  # pip: matplotlib==3.7.0\n",
    "smuggle pandas as pd              # pip: pandas==1.5.3\n",
    "smuggle seaborn as sns            # pip: seaborn==0.12.2\n",
    "smuggle timecorr as tc            # pip: timecorr==0.1.7\n",
    "\n",
    "from sklearn.decomposition smuggle IncrementalPCA as PCA  # pip: scikit-learn==1.2.1\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "datadir = os.path.join(basedir, 'data')\n",
    "figdir = os.path.join(basedir, 'paper', 'figs', 'source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating synthetic datasets\n",
    "\n",
    "We create four datasets, exemplifying each of the following scenarios:\n",
    "  - High informativeness, high compressibility\n",
    "  - High informativeness, low compressibility\n",
    "  - Low informativeness, high compressibility\n",
    "  - Low informativeness, low compressibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell doesn't actually make sense...maybe use the timecorr synthetic data generator?\n",
    "\n",
    "Potential idea:\n",
    "  - Create a multi-subject dataset\n",
    "  - High informativeness = strong correlations across subjects, each timepoint has a unique pattern\n",
    "  - High compressibility = highly similar features (e.g., all constant values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highly informative datasets have a lot of variability across samples\n",
    "# highly compressible datasets have lots of repeated structure within samples\n",
    "\n",
    "buffer = 1000\n",
    "n_features = 10\n",
    "n_obs = 20\n",
    "hi_var = 10\n",
    "lo_var = 0.001\n",
    "\n",
    "# alternating ones and zeros\n",
    "alternating = np.zeros((n_obs, n_features))\n",
    "alternating[1::2, ::2] = 1\n",
    "alternating[::2, 1::2] = 1\n",
    "\n",
    "# highly informative, highly compressible\n",
    "HI_HC = np.cumsum(hi_var * np.random.normal(size=(n_obs + buffer, n_features)), axis=0)[buffer:, :] + (n_obs * hi_var * alternating)\n",
    "\n",
    "# highly informative, low compressibility\n",
    "HI_LC = np.cumsum(hi_var * np.random.normal(size=(n_obs + buffer, n_features)), axis=0)[buffer:, :]\n",
    "\n",
    "# low informativeness, highly compressible\n",
    "LI_HC = np.cumsum(lo_var * np.random.normal(size=(n_obs + buffer, n_features)), axis=0)[buffer:, :] + (n_obs * hi_var * alternating)\n",
    "\n",
    "# low informativeness, low compressibility\n",
    "LI_LC = np.cumsum(lo_var * np.random.normal(size=(n_obs + buffer, n_features)), axis=0)[buffer:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a dataset, compute peak decoding accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
