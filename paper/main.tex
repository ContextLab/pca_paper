\documentclass[english]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{apacite}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{pxfonts}
\usepackage[utf8]{inputenc}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage[left]{lineno}
\usepackage{soul}
\linenumbers


\title{High-order dynamic neural correlations reflect naturalistic processing
in humans} \author{Lucy
  L. W. Owen$^1$, and\
  Jeremy R. Manning\textsuperscript{$1, \dagger$}\\
  [0.1in]$^1$Department of Psychological and Brain
  Sciences,\\Dartmouth
  College, Hanover, NH\\
  \textsuperscript{$\dagger$}Address correspondence to
  jeremy.r.manning@dartmouth.edu}


\begin{document}
\maketitle


\begin{abstract}
We applied dimensionality reduction algorithms to the activity patterns in each experimental condition.  Specifically, we sought to understand the “dimensionality” of the neural patterns that were sufficient to decode participants’ listening times (or approach was similar to that of Mack et al. 2017).  We found that even low-dimensional embeddings of the data were sufficient to accurately decode listening times from the intact story recording, whereas finer temporal scramblings of the story required higher-dimensional embeddings of the data to reach peak decoding accuracy.
\end{abstract}

\doublespacing

\section*{Introduction}

We're interested in the complexity of brain patterns that underly
different types of thoughts. To explore this question space, we take
brain patterns recorded under different experimental conditions and
project them into lower dimensional spaces using principle components
analysis. We can then ask how well those low-dimensional embeddings of
the data retain cognitively relevant information like when in a story
someone is listening to. 
Then we can explore decoding accuracy as a
function of the number of components, or dimensions, in the
low-dimensional space under different cognitive conditions.

You can imagine two reasonable predictions of
how cognition is reflected in brain patterns.  The first is as our
thoughts become more complex, they are supported by more complex brain
patterns, and require more components to decode.  The second is that
when thoughts are deeper and more complicated, the units of neural
activity would carry more information, and would require therefore
fewer components to decode.  

This idea can be explored in this visual analogy (Fig.~\ref{fig: pie_example}) for neural
compression.  Here there are two images of pies, the top pie is more
complex than the bottom.  On the left we're illustrating that it takes
fewer componenets to reach the same 95 percent variance explained in
the less complex pie, which corresponds to higher compression.
However, on the right with very few components similar
variance is explaining both pies.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/pie_example.pdf}
  \caption{\textbf{Illustration of compression.} Visual analogy for
    neural compression. Here are 2 images of pies, one more complex
    thantheother. \textbf{a.}It takes fewer components to reach the same percent
    variance explained in the less complex pie, which corresponds to
    higher compression. \textbf{b.}However, with very few components, similar variance is explained in both pies.
\textbf{c.} Plots the cumulative explained variance for more and more components.}
  \label{fig:pie_example}
\end{figure}


We investigated the dimensionality of neural patterns by training
classifiers using more and more principle components. Or, in other
words, we used less and less compression to decode.
We applied the approach to
a neuroimaging dataset comprising data collected as participants
listened to a story varying in cognitive richness~\citep{SimoEtal16}.






\section*{Methods}

\subsubsection*{Dimensionality reduction}

Explain PCA 


\subsection*{Evaluation metrics}
 We evaluated the degree of
compression of held-out
neuroimaging data with the time at which it was collected.  We used this latter evaluations (using timepoint
decoding) as a proxy for
gauging how much explanatory power the compressed data held
with respect to the observed data.



\subsubsection*{Timepoint decoding}

To explore how higher-order structure varies with stimulus structure
and complexity, we used a previous neuroimaging dataset
~\cite{SimoEtal16}  in which participants listened to an audio recording of a story; 36 particpants listen to an intact version of the story, 17 participants listen to time-scrambled recordings of the same story where paragraphs were scrambled, 36 particpants listen to word-scrambled version and 36 participants lay in rest condition.

Prior work has shown participants share similar neural responses to
richly structured stimuli when compared to stimuli with less
structure.  To assess whether the moment-by-moment higher order
correlations were reliably preserved across participants, we used
inter-subject functional connectivity (ISFC) to isolate the
time-varying correlational structure (functional connectivity patterns
that were specifically driven by the story participants listened to.
Following the analyses conducted by (HTFA)~\cite{MannEtal18}, we first
applied \textit{hierarchical topographic factor analysis} (HTFA) to
the fMRI datasets to obtain a time series of 700 node activities for
every participant.  We then applied dimensionality reduction
(Incremental PCA) for each group.

We then compared the groups’ activity patterns (using Pearson
correlations) to estimate the story times each corresponding pattern
using more and more principle components.   

To assess decoding accuracy, we randomly divided participants for each
stimulus into training and testing groups. We then compared the
groups’ activity patterns (using Pearson correlations) to estimate the
story times each corresponding pattern using more and more principle
components (as the data became less compressed). Specifically, we asked, for each timepoint: what are the correlations
between the first group's and second group's activity patterns at each
order. We note that the decoding test we used is a conservative in which we count a timepoint label as incorrect if it is not an exact match.



\section*{Results}

For our decoding analysis, we used HTFA-derived node activities
~\cite{MannEtal18} from fMRI data collected as participants listened
to an audio recording of a story (intact condition; 36 participants),
listened to time scrambled recordings of the same story (17
participants in the paragraph-scrambled condition listened to the
paragraphs in a randomized order and 36 in the word-scrambled
condition listened to the words in a randomized order), or lay resting
with their eyes open in the scanner (rest condition; 36
participants). We sought to understand the 'dimensionality' of the
neural patterns by using more and more principle
components to decode a multi-subject fMRI datasets. This story listening dataset was
collected as part of a separate study, where the full imaging
parameters, image preprocessing methods, and experimental details may
be found~\citep{SimoEtal16}. The dataset is available at
\href{url}{http://arks. princeton.edu/ark:/88435/dsp015d86p269k}.


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/decode_interpret.pdf}
  \caption{\textbf{Decoding accuracy.} \textbf{a. Decoding accuracy by
      number of components.} Ribbons of each color display
    cross-validated decoding performance for each condition (intact,
    paragraph, word, and rest). Decoders were trained using
    increasingly more principle components and displayed relative to
    chance (red line). \textbf{b. Fixed decoding accuracy by number of
      components.} We zoom in on the plot shown in \textbf{a.} and add
    a line denoting fixed decoding accuracy (.05). We plot where the
    intact, paragraph, and word conditions intersect.
    \textbf{c. Explanation of inflection metric.} First the we fit a sigmoid function to the decoding accuracy by number of components. Second, we found where the second derivate is both positive and less than .0001. Last, we then plot that inflection point as a single metric to capture the slope and asymptote of the curve.}
  \label{fig:decode_interpret}
\end{figure}

We performed a decoding analysis, using cross validation to estimate
(using other participants’ data) which parts of the story the additional added
principle component corresponded to (see \textit{Materials and methods}). We note that our primary goal was not to achieve perfect decoding accuracy, but rather to use decoding accuracy as a benchmark for assessing whether different neural features specifically capture cognitively relevant brain patterns.

Separately for each experimental condition, we divided participants
into two groups. Starting with 1 principle comonent, for
each dimension we added another principle component, and we correlated the group 1 activity patterns with group 2
activity patterns.  There were 272
timepoints for paragraph condition, 300 timepoints for intact and word
conditions, and 400 timepoints for rest condition,  so chance
performance on this decoding test is was $\frac{1}{272}$,
$\frac{1}{300}$, and $\frac{1}{400}$ respectively.
 
- As complexity of the stimulus increases, decoding accuracy
increases (Fig.~\ref{fig:decode_interpret},  a.).  





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Discussion}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/direction_of_field.pdf}
  \caption{\textbf{Direction of the field (adapted from~\citep{Turk13}).} The evolution of fMRI analyses started with
    univariate activation, which refers to
    the average amplitude of BOLD activity evoked by events of an
    experimental condition. Next, multivariate classifiers are trained
    on patterns of activation across voxels to decode distributed
    representations for specific events. The next, resting
    connectivity, is the temporal correlation of one or more seed
    regions with the remainder of the brain during rest. Additionally,
    task-based connectivity examines how these correlations differ by
    cognitive state. Following this increasing trajectory of
    increasing complexity, full connectivity considers all pairwise
    correlations in the brain, most commonly at rest.  Next, dynamic
    full connectivity considers how full connectivity changes over
    time. Continuing this line of reasoning, we expect higher-order network dynamics might provide even richer insights into the neural basis of cognition.}
  \label{fig:direction_of_field}
\end{figure}



Based on prior work ~\citep{Deme19} and following the direction of the field ~\citep{Turk13} we think our thoughts might be encoded in
dynamic network patterns, and possibly higher order network
patterns (Fig.~\ref{fig:direction_of_field}). We sought to test this hypothesis by developing an approach
to inferring high-order network dynamics from timeseries data. 

One challenge in studying dynamic interactions is the
computational resources required to calculate higher-order correlations. 
We developed a computationally tractable model of network dynamics (Fig.~\ref{fig:methods_fig}) that takes in a feature
timeseries and outputs approximated first-order dynamics (i.e.,
dynamic functional correlations), second-order dynamics
(reflecting homologous networks that dynamically form and disperse),
and higher-order network dynamics (up to tenth-order dynamic
correlations).

We first validated our model using synthetic data, and explored how
recovery varied with different underlying data structures and kernels.   We then 
applied the approach to an fMRI dataset
~\citep{SimoEtal16} in which participants listened to an audio
recording of a story, as well as scrambled versions of the same story
(where the scrambling was applied at different temporal scales).  We
trained classifiers to take the output of the model and decode the
timepoint in the story (or scrambled story) that the participants were
listening to. We found that, during the intact listening condition in the
experiment, classifiers that incorporated higher-order correlations
yielded consistently higher accuracy than classifiers trained only on
lower-order patterns (Fig.~\ref{fig:decoding_level},  a.\&d.).  By contrast, these
higher-order correlations were not necessary to support decoding the other
listening conditions and (minimally
above chance) during a control rest condition.  This suggests
that the cognitive processing that supported the most cogntively rich listening conditions
involved second-order (or higher) network dynamics.

Although we found decoding accuracy was best when incorporating
higher-order network dynamics for all but rest
  condition, it is unclear if this is a product of the brain or the
  data collection technique.  It could be that the brain is
  second-order or it could be that fMRI can
  only reliably give second-order interactions. Exploring this method
  with other data collection technique will be important to
  disentangle this question.



  \subsection*{Concluding remarks}

How can we better understand how brain patterns change over
time? How can we quantify the potential network dynamics that might be
driving these changes? One way to judge the techniques of the future is
to look at the trajectory of the fMRI field so far has taken so far
(Fig.~\ref{fig:methods_fig}).  The field started with 
univariate activation, measuring the average activity for each voxel.
Analyses of multivariate activation followed, looking at spatial patterns of
activity over voxels. Next, correlations of activity were explored, first
with measures like resting connectivity that take temporal correlation
between a seed voxel and all other voxels then with full connectivty
that measure all pairwise correlations.  Additionally, this path of increasing
complexity also moved from static to dynamic measurements.  One
logical next step in this trajectory would be dynamic higher-order
correlations. We have created a method 
to support these calculations by scalably approximating dynamic higher-order
correlations.  

\section*{Acknowledgements}
We acknowledge discussions with Luke Chang, Hany Farid, Paxton
Fitzpatrick, Andrew Heusser, Eshin Jolly, Qiang Liu, Matthijs van der
Meer, Judith Mildner, Gina Notaro, Stephen Satterthwaite, Emily
Whitaker, Weizhen Xie, and Kirsten Ziman. Our work was supported in
part by NSF EPSCoR Award Number 1632738 to J.R.M. and by a sub-award
of DARPA RAM Cooperative Agreement N66001-14-2-4-032 to J.R.M.  The
content is solely the responsibility of the authors and does not
necessarily represent the official views of our supporting
organizations.

\section*{Author contributions}
Concept: J.R.M.  Implementation: T.H.C., L.L.W.O., and
J.R.M.  Analyses: L.L.W.O and J.R.M.

\bibliographystyle{apacite}
\bibliography{CDL-bibliography/memlab}

\end{document}


