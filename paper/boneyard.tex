
By training classifiers using more and more principle components to decode, and comparing across conditions with varying degrees of cognitive richness, we can assess the explanatory power of the compressed data held with respect to the observed data (see \textit{Methods}). We note that our primary goal was not to achieve perfect decoding accuracy, but rather to use decoding accuracy as a benchmark for assessing whether different neural features specifically capture cognitively relevant brain patterns.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figs/decode_interpret.pdf}
  \caption{\textbf{Decoding accuracy.} \textbf{a. Decoding accuracy by
      number of components.} Ribbons of each color display
    cross-validated decoding performance for each condition (intact,
    paragraph, word, and rest). Decoders were trained using
    increasingly more principle components and displayed relative to
    chance (red line). \textbf{b. Fixed decoding accuracy by number of
      components.} We zoom in on the plot shown in \textbf{a.} and add
    a line denoting fixed decoding accuracy (.05). We plot where the
    intact, paragraph, and word conditions intersect.
    \textbf{c. Explanation of inflection metric.} First the we fit a sigmoid function to the decoding accuracy by number of components. Second, we found where the second derivative is both positive and less than .0001. Last, we then plot that inflection point as a single metric to capture the slope and asymptote of the curve.}
    \label{fig:decode_interpret}
  \end{figure}




% We performed a decoding analysis, using cross validation to estimate
% (using other participants’ data) which parts of the story the additional added
% principle component corresponded to (see \textit{Materials and methods}). We note that our primary goal was not to achieve perfect decoding accuracy, but rather to use decoding accuracy as a benchmark for assessing whether different neural features specifically capture cognitively relevant brain patterns.

% Separately for each experimental condition, we divided participants
% into two groups. Starting with 1 principle comonent, for
% each dimension we added another principle component, and we correlated the group 1 activity patterns with group 2
% activity patterns.  There were 272
% timepoints for paragraph condition, 300 timepoints for intact and word
% conditions, and 400 timepoints for rest condition,  so chance
% performance on this decoding test is was $\frac{1}{272}$,
% $\frac{1}{300}$, and $\frac{1}{400}$ respectively.
 
% - As complexity of the stimulus increases, decoding accuracy
% increases (Fig.~\ref{fig:decode_interpret},  a.).  Replication of
% Simony findings. 

% - As complexity of the stimuli increases, we need fewer components to
% decode the same amount (Fig.~\ref{fig:decode_interpret},  b.).  

% - As complexity of the stimuli increases, more components are required to reach peak decoding accuracy (Fig.~\ref{fig:decode_interpret},  c.).  




Prior work has shown participants share similar neural responses to
richly structured stimuli when compared to stimuli with less
structure \cite{SimoEtal16}.  We replicate this finding, showing as complexity of the stimulus increases, decoding accuracy
increases (Fig.~\ref{fig:decode_interpret},  a.).  
Additionally, we found that as complexity of the stimuli increases, we need fewer components to decode the same amount (Fig.~\ref{fig:decode_interpret},  b.). However, we also found that as complexity of the stimuli increases, more components are required to reach peak decoding accuracy (Fig.~\ref{fig:decode_interpret},  c.).  
We posit that as the complexity of our thoughts increases, neural
compression decreases. However, as our thoughts become deeper and richer, more reliable information is available at higher neural compression.


% % PCA, or the closely related factor analysis and singular value decomposition (SVD) (Hastie et al., 2009), is widely used in the study of individual differences and aids estimating how many latent components, or “factors”, underlie a pattern of (item) responses within or across participants, as for instance in the context of intelligence (Spearman, 1904) or personality tests (Cattell, 1947). In the context of neuroimaging, PCA has been used to identify brain networks (Huth et al., 2012; Friston et al., 1993). 

%  % add citation:
% % S.M.Smith,A.Hyva ̈rinen,G.Varoquaux,K.L.Miller,and C. F. Beckmann, “Group-PCA for very large fMRI datasets,” NeuroImage, vol. 101, pp. 738, 2014.


% \subsection*{Evaluation metrics}
%  We will evaluate the degree of
% compression of held-out
% neuroimaging data by assessing the time at which it was collected.  We will use this evaluation (timepoint
% decoding) as a proxy for
% gauging how much explanatory power the compressed data held
% with respect to the observed data.



% \subsubsection*{Timepoint decoding}

% To explore how compression varies with complexity, we will use a previous neuroimaging dataset
% ~\cite{SimoEtal16}  in which participants listened to an audio recording of a story; 36 particpants listen to an intact version of the story, 17 participants listen to time-scrambled recordings of the same story where paragraphs were scrambled, 36 particpants listen to word-scrambled version and 36 participants lay in rest condition.


% Following the analyses conducted by (HTFA)~\cite{MannEtal18}, we first
% apply \textit{hierarchical topographic factor analysis} (HTFA) to
% the fMRI datasets to obtain a time series of 700 node activities for
% every participant.  We then apply dimensionality reduction
% (Incremental PCA) for each group.

% We then compare the groups’ activity patterns (using Pearson
% correlations) to estimate the story times each corresponding pattern
% using more and more principle components.   

% To assess decoding accuracy, we randomly divide participants for each
% stimulus into training and testing groups. We then compare the
% groups’ activity patterns (using Pearson correlations) to estimate the
% story times each corresponding pattern using more and more principle
% components (as the data became less compressed). Specifically, we ask, for each timepoint: what are the correlations
% between the first group's and second group's activity patterns at each
% order. We note that the decoding test we used is a conservative in which we count a timepoint label as incorrect if it is not an exact match.





% \subsection*{Evaluation metrics}
%  We evaluated the degree of
% compression of held-out
% neuroimaging data with the time at which it was collected.  We used this latter evaluations (using timepoint
% decoding) as a proxy for
% gauging how much explanatory power the compressed data held
% with respect to the observed data.



% \subsubsection*{Timepoint decoding}

% To explore how higher-order structure varies with stimulus structure
% and complexity, we used a previous neuroimaging dataset
% ~\cite{SimoEtal16}  in which participants listened to an audio recording of a story; 36 particpants listen to an intact version of the story, 17 participants listen to time-scrambled recordings of the same story where paragraphs were scrambled, 36 particpants listen to word-scrambled version and 36 participants lay in rest condition.

% Prior work has shown participants share similar neural responses to
% richly structured stimuli when compared to stimuli with less
% structure.  To assess whether the moment-by-moment higher order
% correlations were reliably preserved across participants, we used
% inter-subject functional connectivity (ISFC) to isolate the
% time-varying correlational structure (functional connectivity patterns
% that were specifically driven by the story participants listened to.
% Following the analyses conducted by (HTFA)~\cite{MannEtal18}, we first
% applied \textit{hierarchical topographic factor analysis} (HTFA) to
% the fMRI datasets to obtain a time series of 700 node activities for
% every participant.  We then applied dimensionality reduction
% (Incremental PCA) for each group.

% We then compared the groups’ activity patterns (using Pearson
% correlations) to estimate the story times each corresponding pattern
% using more and more principle components.   

% To assess decoding accuracy, we randomly divided participants for each
% stimulus into training and testing groups. We then compared the
% groups’ activity patterns (using Pearson correlations) to estimate the
% story times each corresponding pattern using more and more principle
% components (as the data became less compressed). Specifically, we asked, for each timepoint: what are the correlations
% between the first group's and second group's activity patterns at each
% order. We note that the decoding test we used is a conservative in which we count a timepoint label as incorrect if it is not an exact match.



  
  Most complex systems reflect dynamic interactions between myriad
  evolving components (e.g., interacting molecules, interacting brain
  systems, interacting individuals within a social network or
  ecological system, coordinated components within a mechanical or
  digital device, etc.).  Despite that these interactions are central
  to the full system's behavior (e.g., removing a component from the
  full system can change the entire system's behavior), dynamic
  interactions cannot typically be directly measured.  Rather, the
  interactions must be inferred through their hypothesized role in
  guiding the dynamics of system components.  Here we use a
  model-based approach to inferring dynamic interactions from
  timeseries data.  In addition to examining first-order interactions
  (e.g., between pairs of components) we also examine higher-order
  interactions (e.g., that characterize mirrored structure in the
  patterns of interaction dynamics displayed by different subsets of
  components).  We apply our approach to two datasets.  First, we use
  a synthetic dataset, for which the underlying dynamic interactions
  are known, to show that our model recovers those ground-truth
  dynamic interactions.  We also apply our model to a neuroimaging
  dataset and show that the high-order dynamic interactions exhibited
  by brain data vary meaningfully as a function of the cognitive
  ``richness'' of the stimulus people are experiencing.



%should we introduce the name levelup in the introduction? ie. "in the second step, levelup, we use..." -ecw
%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Methods}
%%%%%%%%%%%%%%%%%%%%%%%%

The mathematical theories behind the timecorr approach for calculating the moment-by-moment dynamic correlations in systems are described here. The function takes a number-of-timepoint by number-of-features matrix as well as a weight function to determine a moment-by-moment correlations matrix. To calculate the correlations, expectation values are found using equation 1: 

\begin{equation}
\mathrm{corr}_{x,y}(t) = \frac{1}{T} \sum_{t} \frac{\left(x(t)-\mathbb{E}[x]\right)\left(y(t)-\mathbb{E}[y]\right)}{\sigma_{x}(t)\sigma_{y}(t)}
\end{equation}

%should the variables in this equation/description change to match the description in the intro?
Here, x is the timepoint component of the matrix and y is the feature component. Timepoint by timepoint, x by x, at each given moment (t) in the matrix is subtracted from the expectation value of the full timepoint matrix. The same process occurs for the features points (y) at each timepoint (t). The product of these are then divided by the standard deviation at that moment for both the timepoint and the feature. Only the upper triangle of the matrix is calculated as the interactions are assumed to be both symmetric and non-recurrent. 

Two weight parameters have been used to calculate the connectivity of the data are a Gaussian weight parameter, equation 2, and a LaPlace weights parameter, equation 3. 

%label as Gaussian? 
\begin{equation}
\mathcal{N}\left( x | \mu, \sigma \right) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{|| x - \mu ||^2}{2\sigma^2}\right)
\end{equation}
where sigma squared is the variance and mu is the expected value. 

%label as LaPlace? 
\begin{equation}
\mathcal{L}\left( x | \mu, \lambda \right) = \frac{1}{2 \lambda}\left( x | \mu, \lambda \right) = \exp\left(-\frac{|x - \mu|}{\lambda}\right)
\end{equation}
where mu is the expectation values, lambda is a defined decay constant. 

%figure showing the difference b/w two weights in same data set? Using the synthetic data?

When using the Gaussian weights parameter timepoints are more closely correlated to timepoints further away in the matrix than when using then the LaPlace weight function.

%Plot of kernels as Fig 1?

% New section? Mathematical Methods of Levelup? 
In order to understand how the data is interconnected we use the function levelup. Levelup uses the same input as the timecorr function, a timepoint-by-function matrix. 

%Diagram of input-->timecorr-->PCaA= levelup?

In the levelup method the the data first goes through the timecorr process and then is reduced using Principal Component Analysis (PCA). By utilizing PCA the output data maintains the same features as the input method. Not only does this allow for a sanity check, but also it allows for repeated use of the function. Using PCA or other reduction techniques is needed as without it the output data would get exponentially large. On the flip side using PCA, or any dimensionality reduction technique, makes the output data less accurate as information is lost. 


%PCA equation?
%figure/ chart of how many levels we "go up" before data gets too muddled- depends on the correlation?


Timepoints that share high similarity across all subjects have a high probability of being stimulus driven.

To compute the correlations of multiple subjects the across mode of both timecorr and levelup are used. The across function compares one voxel in a sample to the averages of that voxel in the rest of the dataframe. This occurs throughout every voxel in the dataset. The output of the across mode is a correlation matrix that is a number of timepoints by features matrix.

%F^2-F/2 +F 
